- name: Install common packages
  become: yes
  hosts:
    - masters
    - nodes
    - nfs
    - jump_host
  roles:
    - pebbles_facts
  tasks:
    - name: install common packages
      dnf:
        state: present
        name:
          - sysstat
          - python3
          - dstat
          - vim
          - tree

    - name: Start and enable sysstat
      systemd:
        name: sysstat
        state: started
        enabled: yes

- hosts: masters
  gather_facts: yes
  become: yes
  roles:
    - pebbles_facts

    # get installation status to figure out if we should skip volume setup
    - role: installation_status
      vars:
        operation: get

    # configure volumes, only for fresh hosts
    - name: volumes
      vars:
        vol_k3slib_device: "{{ k3slib_device|d() }}"
        vol_backup_device: "{{ backup_device|d() }}"
        vol_image_device: "{{ image_device|d() }}"
      when:
        - pb_installed | d(false) | bool == false

- hosts: masters
  gather_facts: yes
  become: yes
  tasks:

    - name: mount tmpfs for K3s server directory to keep database and manifest contents in RAM only
      become: True
      mount:
        name: /var/lib/rancher/k3s/server
        src: tmpfs
        fstype: tmpfs
        # todo: add 'noswap' when the kernel supports it
        opts: size=512M,uid=0,gid=0,mode=0700
        state: mounted
      when:
        - k3s_server_on_ramdisk | d(false) | bool == true

    # See scripts/create_k3s_master_boostrap_data.bash for more info
    # Use /var/lib/rancher/k3s/server/cred as an indicator of an already bootstrapped system
    - name: restore master bootstrap data
      shell: >
        echo "{{ vaulted_master_bootstrap_data }}"
        | base64 --decode
        | tar xvz -C /
      args:
        creates: /var/lib/rancher/k3s/server/cred
      when:
        - vaulted_master_bootstrap_data is defined
        - restore_master_bootstrap_data | default(false) | bool

- hosts: masters
  gather_facts: yes
  become: yes
  vars:
    master_ip: "{{ master_public_ip }}"
  roles:
    - pebbles_facts
    - k3s

- import_playbook: fetch_k3s_kubeconfig.yml

- hosts: nfs
  become: yes
  pre_tasks:
    - name: check if /etc/exports already has exported entries
      stat:
        path: "/etc/exports"
      register: stat_etc_exports

    - name: force ansible_os_family for nfs role
      set_fact:
        ansible_os_family: 'RedHat'
  roles:
    - name: pebbles_facts

    # get installation status to figure out if we should skip volume setup
    - role: installation_status
      vars:
        operation: get

    # configure volumes, only for fresh hosts
    - name: volumes
      vars:
        vol_nfs_device: "{{ nfs_device|d() }}"
      when:
        - pb_installed | d(false) | bool == false

    # Apply nfs role only if exports file is empty or does not exist. The role would overwrite it.
    - name: nfs
      when: not stat_etc_exports.stat.exists or stat_etc_exports.stat.size==0

    - name: nfs_provisioner

    # Deploy node_exporter on NFS server using prometheus community galaxy role
    - name: prometheus.prometheus.node_exporter

  tasks:
    - name: start and enable nfs-server (nfs role won't do it if there are no exports)
      systemd:
        name: nfs-server
        state: started
        enabled: yes

- hosts: nodes
  gather_facts: yes
  become: yes
  roles:
    - pebbles_facts

    # get installation status to figure out if we should skip volume setup
    - role: installation_status
      vars:
        operation: get

    # configure volumes, only for fresh hosts
    - name: volumes
      vars:
        vol_k3slib_device: "{{ k3slib_device|d() }}"
      when:
        - pb_installed | d(false) | bool == false

    - k3s

- hosts: localhost
  gather_facts: no
  roles:
    - pebbles_facts
  tasks:
    - name: wait for nodes to be ready
      shell: >
        oc get node {{ item }}
      register: result
      until: result.stdout.find(' Ready ') != -1
      retries: 30
      delay: 5
      changed_when: false
      with_items: "{{ groups.nodes }}"

    - name: label kube-system namespace for network policy selection
      k8s:
        kind: Namespace
        api_version: v1
        name: kube-system
        definition:
          metadata:
            labels:
              name: kube-system

    - name: label nodes for user workloads, labels
      k8s:
        kind: Node
        api_version: v1
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              role: user
      with_items: "{{ groups.nodes }}"

    - name: label nodes for user workloads, taints
      k8s:
        kind: Node
        api_version: v1
        name: "{{ item }}"
        definition:
          spec:
            taints:
              - effect: NoSchedule
                key: role
                value: user
      with_items: "{{ groups.nodes }}"
      when: taint_nodes_for_user_workloads | d(true) | bool

    - name: add nfs-1 storage class
      k8s:
        kind: StorageClass
        api_version: storage.k8s.io/v1
        name: nfs-1
        definition:
          provisioner: notebooks.csc.fi/nfs-1
          mountOptions:
            - vers=4.1

- import_playbook: deploy_k3s_manifests.yml

- hosts: masters
  become: yes
  tasks:
    - name: install sqlite3
      dnf:
        name: sqlite
        state: present

    - name: setup script for sqlite vacuuming
      copy:
        dest: /usr/local/sbin/k3s_sqlite_vacuum.sh
        owner: root
        group: root
        mode: 0700
        content: |
          #!/usr/bin/env sh
          echo "vacuum; pragma wal_checkpoint(truncate)" | sqlite3 /var/lib/rancher/k3s/server/db/state.db

    - name: setup cron for sqlite vacuuming
      ansible.builtin.cron:
        name: k3s sqlite vacuuming
        minute: "0"
        hour: "3"
        user: root
        job: "/usr/local/sbin/k3s_sqlite_vacuum.sh"
        cron_file: ansible_k3s_sqlite_vacuuming

    - name: set up a script for sqlite backups
      copy:
        dest: /usr/local/sbin/k3s_sqlite_backup.sh
        owner: root
        group: root
        mode: 0700
        content: |
          #!/usr/bin/env sh

          bu_file=/var/lib/pb/backup/k3s_sqlite_$(date -Is).db
          # run backup on k3s sqlite
          sqlite3 /var/lib/rancher/k3s/server/db/state.db ".backup ${bu_file}"

          # compress the backup
          gzip ${bu_file}

    - name: set up a cron job for sqlite backups
      ansible.builtin.cron:
        name: k3s sqlite backup
        minute: "5"
        hour: "3"
        user: root
        job: "/usr/local/sbin/k3s_sqlite_backup.sh"
        cron_file: ansible_k3s_sqlite_backup

    - name: set up a script for K3s object dump
      copy:
        dest: /usr/local/sbin/k3s_object_dump.sh
        owner: root
        group: root
        mode: 0700
        content: |
          #!/usr/bin/env sh

          bu_file=/var/lib/pb/backup/k3s_objects_$(date -Is).yaml.gz
          # dump objects from K3s
          /usr/local/bin/kubectl get --all-namespaces -o yaml all,pv,pvc | gzip > ${bu_file}

    - name: set up a cron job for K3s object dumps
      ansible.builtin.cron:
        name: k3s object dump
        minute: "45"
        hour: "3"
        user: root
        job: "/usr/local/sbin/k3s_object_dump.sh"
        cron_file: ansible_k3s_object_dump

- hosts: localhost
  tasks:
    - name: Cleanup traefik in compute nodes
      k8s:
        state: absent
        api_version: v1
        kind: Pod
        namespace: kube-system
        name: "{{ item['metadata']['name'] }}"
      with_items: "{{ lookup('k8s', kind='Pod', namespace='kube-system', label_selector='app=svclb-traefik') }}"
      when:
        - item.spec is defined # checks if returned with only one item in a list
        - item['spec']['nodeName'] != cluster_name + '-master'

- name: Set installation state
  become: yes
  hosts:
    - masters
    - nodes
    - nfs
  roles:
    - pebbles_facts
    - role: installation_status
      vars:
        operation: set
        version_data: "{{ repo_version_data }}"
