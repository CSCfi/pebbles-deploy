- hosts: masters
  gather_facts: yes
  become: yes
  roles:
    - pebbles_facts

    - name: volumes
      vars:
        vol_k3srun_device: "{{ k3srun_device|d() }}"
        vol_backup_device: "{{ backup_device|d() }}"
        vol_image_device: "{{ image_device|d() }}"

- hosts: masters
  gather_facts: yes
  become: yes
  tasks:
    # use /var/lib/rancher as an indicator of an already bootstrapped system
    - name: restore master bootstrap data
      shell: >
        echo "{{ vaulted_master_bootstrap_data }}"
        | base64 --decode
        | tar xvz -C /
      args:
        creates: /var/lib/rancher
      when:
        - vaulted_master_bootstrap_data is defined
        - restore_master_bootstrap_data | default(false) | bool

- hosts: masters
  gather_facts: yes
  become: yes
  vars:
    master_ip: "{{ master_public_ip }}"
  roles:
    - pebbles_facts
    - k3s

- import_playbook: fetch_k3s_kubeconfig.yml

- hosts: nfs
  become: yes
  pre_tasks:
    - name: check if /etc/exports already has exported entries
      stat:
        path: "/etc/exports"
      register: stat_etc_exports
  roles:
    - name: pebbles_facts
    - name: volumes
      vars:
        vol_nfs_device: "{{ nfs_device|d() }}"
    # Apply nfs role only if exports file is empty or does not exist. The role overrides it.
    - name: nfs
      when: not stat_etc_exports.stat.exists or stat_etc_exports.stat.size==0
    - name: nfs_provisioner
  tasks:
    - name: start and enable nfs-server (nfs role won't do it if there are no exports)
      systemd:
        name: nfs-server
        state: started
        enabled: yes

- hosts: nodes
  gather_facts: yes
  become: yes
  roles:
    - pebbles_facts
    - name: volumes
      vars:
        vol_k3srun_device: "{{ k3srun_device|d() }}"
    - k3s

- hosts: masters, nodes
  gather_facts: yes
  become: yes
  roles:
    - pebbles_facts
  tasks:
    - name: create custom local storage directory
      file:
        dest: "{{ custom_local_storage_dir }}"
        state: directory
        seuser: system_u
        serole: object_r
        setype: container_file_t
        selevel: s0
      when:
        - custom_local_storage_dir is defined

- hosts: localhost
  gather_facts: no
  roles:
    - pebbles_facts
  tasks:
    - name: wait for nodes to be ready
      shell: >
        oc get node {{ item }}
      register: result
      until: result.stdout.find(' Ready ') != -1
      retries: 30
      delay: 5
      changed_when: false
      with_items: "{{ groups.nodes }}"

    - name: label kube-system namespace for network policy selection
      k8s:
        kind: Namespace
        api_version: v1
        name: kube-system
        definition:
          metadata:
            labels:
              name: kube-system

    - name: label nodes for user workloads
      k8s:
        kind: Node
        api_version: v1
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              role: user
      with_items: "{{ groups.nodes }}"

    - name: label nodes for user workloads
      k8s:
        kind: Node
        api_version: v1
        name: "{{ item }}"
        definition:
          spec:
            taints:
              - effect: NoSchedule
                key: role
                value: user
      with_items: "{{ groups.nodes }}"
      when: taint_nodes_for_user_workloads | d(true) | bool

    - name: add nfs-1 storage class
      k8s:
        kind: StorageClass
        api_version: storage.k8s.io/v1
        name: nfs-1
        definition:
          provisioner: notebooks.csc.fi/nfs-1
          mountOptions:
            - vers=4.1

- hosts: masters
  become: yes
  tasks:
    - name: custom manifest templates
      template:
        src: "/opt/deployment/pebbles-environments/data/k3s-manifests/{{ item }}"
        dest: "/var/lib/rancher/k3s/server/manifests/{{ item | splitext | first }}"
        owner: root
        group: root
        mode: 0644
      when: item | splitext | last == '.j2'
      with_items: "{{ custom_manifests|d([ ]) }}"

    - name: custom manifest files
      copy:
        src: "/opt/deployment/pebbles-environments/data/k3s-manifests/{{ item }}"
        dest: "/var/lib/rancher/k3s/server/manifests/{{ item }}"
        owner: root
        group: root
        mode: 0644
      when: item | splitext | last in ('.yaml', '.yml')
      with_items: "{{ custom_manifests|d([ ]) }}"

    - name: install sqlite3
      dnf:
        name: sqlite
        state: present

    - name: setup script for sqlite vacuuming
      copy:
        dest: /usr/local/sbin/k3s_sqlite_vacuum.sh
        owner: root
        group: root
        mode: 0700
        content: |
          #!/usr/bin/env sh
          echo "vacuum; pragma wal_checkpoint(truncate)" | sqlite3 /var/lib/rancher/k3s/server/db/state.db

    - name: setup cron for sqlite vacuuming
      ansible.builtin.cron:
        name: k3s sqlite vacuuming
        minute: "0"
        hour: "3"
        user: root
        job: "/usr/local/sbin/k3s_sqlite_vacuum.sh"
        cron_file: ansible_k3s_sqlite_vacuuming

- hosts: localhost
  tasks:
    - name: Cleanup traefik in compute nodes
      k8s:
        state: absent
        api_version: v1
        kind: Pod
        namespace: kube-system
        name: "{{ item['metadata']['name'] }}"
      with_items: "{{ lookup('k8s', kind='Pod', namespace='kube-system', label_selector='app=svclb-traefik') }}"
      when:
        - item.spec is defined # checks if returned with only one item in a list
        - item['spec']['nodeName'] != cluster_name + '-master'

- name: Set installation state
  become: yes
  hosts:
    - masters
    - nodes
    - nfs
  roles:
    - pebbles_facts
    - role: installation_status
      vars:
        operation: set
        version_data: "{{ repo_version_data }}"
